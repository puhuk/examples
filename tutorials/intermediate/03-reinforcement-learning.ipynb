{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCS-d1T3znj2"
   },
   "source": [
    "<!-- ---\n",
    "title: Reinforcement Learning with Ignite\n",
    "date: 2021-11-22\n",
    "downloads: true\n",
    "weight: 3\n",
    "tags:\n",
    "  - RL\n",
    "  - intermediate\n",
    "--- -->\n",
    "# Reinforcement Learning with Ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjZMYxFoznj9"
   },
   "source": [
    "In this tutorial we will implement a [policy gradient based algorithm](http://www.scholarpedia.org/article/Policy_gradient_methods) called [Reinforce](http://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf) and use it to solve OpenAI's [Cartpole problem](https://github.com/openai/gym/wiki/CartPole-v0) using PyTorch-Ignite.\n",
    "<!--more--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cT-Tnhdm5sRk"
   },
   "source": [
    "## Prerequisite\n",
    "\n",
    "The reader should be familiar with the basic concepts of Reinforcement Learning like state, action, environment, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLCzuCbjVDoR"
   },
   "source": [
    "## The Cartpole Problem\n",
    "\n",
    "We have to balance a Cartpole which is a pole-like structure attached to a cart. The cart is free to move across the frictionless surface. We can balance the cartpole by moving the cart left or right in 1D. Let's start by defining a few terms.\n",
    "\n",
    "### State\n",
    "\n",
    "There are 4 variables on which the environment depends: cart position and velocity, pole position and velocity.\n",
    "\n",
    "### Action space\n",
    "\n",
    "There are 2 possible actions that the agent can perform: left or right direction.\n",
    "\n",
    "### Reward\n",
    "\n",
    "For each instance of the cartpole not toppling down or going out of range, we have a reward of 1.\n",
    "\n",
    "### When is it solved?\n",
    "\n",
    "The problem is considered solved when the average reward is greater than `reward_threshold` defined for the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sovYyC0Zznj-"
   },
   "source": [
    "## Required Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enczLgLTznkH"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium pytorch-ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aU0KIAEcDeA"
   },
   "source": [
    "### On Colab\n",
    "\n",
    "We need additional dependencies to render the environment on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oC7TJRilfoS"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python-opengl\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEFZUczFRm0d"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ao9uGo-db8T"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT9ro74aTVUf"
   },
   "source": [
    "## Configurable Parameters\n",
    "\n",
    "We will use there values later in the tutorial at appropriate places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biK79K4_MupY"
   },
   "outputs": [],
   "source": [
    "seed_val = 543\n",
    "gamma = 0.99\n",
    "log_interval = 100\n",
    "max_episodes = 1000000\n",
    "render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zux1Dv5Tchc"
   },
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "Let's load our environment first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTfYSODJTbux"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afRSrabwTq_a"
   },
   "source": [
    "### On Colab\n",
    "\n",
    "If on Google Colab, we need to follow a list of steps to render the output. First we initialize our screen size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbhZwa-JM7ub",
    "outputId": "223bdc5b-a057-40ee-8278-522ec1d9184b"
   },
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8qDfZi_R2xW"
   },
   "source": [
    "Below we have a utility function to enable video recording of the gym environment. To enable video, we have to wrap our environment in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2bZH987tHtT"
   },
   "outputs": [],
   "source": [
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env\n",
    "\n",
    "env = wrap_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax8ToOAviGGS"
   },
   "source": [
    "## Model\n",
    "\n",
    "We are going to utilize the reinforce algorithm in which our agent will use episode samples from starting state to goal state directly from the environment. Our model has two linear layers with 4 in features and 2 out features for 4 state variables and 2 actions respectively. We also define an action buffer as `saved_log_probs` and a rewards one. We also have an intermediate ReLU layer through which the outputs of the 1st layer are passed to receive the score for each action taken. Finally, we return a list of probabilities for each of these actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVS15FELytIj"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAKv4jUmrnbj"
   },
   "source": [
    "And then we initialize our model, optimizer, epsilon and timesteps.\n",
    "> TimeStep is the object which contains information about a state like current observation, type of the step, reward, and discount. Given that some action is performed on some state, it gives the new state, type of the new step (or state), discount, and reward achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1Hb2LQveKR6"
   },
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "timesteps = range(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elcEG2gojTsI"
   },
   "source": [
    "## Create Trainer\n",
    "\n",
    "Ignite's [`Engine`](https://pytorch.org/ignite/concepts.html#engine) allows users to define a `process_function` to run one episode. We select an action from the policy, then take the action through `step()` and finally increment our reward. If the problem is solved, we terminate training and save the `timestep`.\n",
    "\n",
    "> An episode is an instance of a game (or life of a game). If the game ends or life decreases, the episode ends. Step, on the other hand, is the time or some discrete value which increases monotonically in an episode. With each change in the state of the game, the value of step increases until the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4ncIcYcznkQ"
   },
   "outputs": [],
   "source": [
    "def run_single_timestep(engine, timestep):\n",
    "    observation = engine.state.observation\n",
    "    action = select_action(policy, observation)\n",
    "    engine.state.observation, reward, done, _, _ = env.step(action)\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    policy.rewards.append(reward)\n",
    "    engine.state.ep_reward += reward\n",
    "\n",
    "    if done:\n",
    "        engine.terminate_epoch()\n",
    "        engine.state.timestep = timestep\n",
    "\n",
    "trainer = Engine(run_single_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfHpIGCEpk0Q"
   },
   "source": [
    "Next we need to select an action to take. After we get a list of probabilities, we create a categorical distribution over them and sample an action from that. This is then saved to the action buffer and the action to take is returned (left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2xxebxhd9J8"
   },
   "outputs": [],
   "source": [
    "def select_action(policy, observation):\n",
    "    state = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV6he1cvuFcw"
   },
   "source": [
    "We initialize a list to save policy loss and true returns of the rewards returned from the environment. Then we calculate the policy losses from the advantage (`-log_prob * reward`). Finally, we reset the gradients, perform backprop on the policy loss and reset the rewards and actions buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8pctWff5Ev6"
   },
   "outputs": [],
   "source": [
    "def finish_episode(policy, optimizer, gamma):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwuJs3n4tEe9"
   },
   "source": [
    "## Attach handlers to run on specific events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPiGVM8FwRSC"
   },
   "source": [
    "We rename the start and end epoch events for easy understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K19lOcPeFF4"
   },
   "outputs": [],
   "source": [
    "EPISODE_STARTED = Events.EPOCH_STARTED\n",
    "EPISODE_COMPLETED = Events.EPOCH_COMPLETED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igNnguT3wWqu"
   },
   "source": [
    "Before training begins, we initialize the reward in `trainer`'s state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNoGCUe9j6EF"
   },
   "outputs": [],
   "source": [
    "trainer.state.running_reward = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB-tEz9awzoj"
   },
   "source": [
    "When an episode begins, we have to reset the environment's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nN3P6Zq6wVo_"
   },
   "outputs": [],
   "source": [
    "@trainer.on(EPISODE_STARTED)\n",
    "def reset_environment_state():\n",
    "    torch.manual_seed(seed_val + trainer.state.epoch)\n",
    "    trainer.state.observation, _ = env.reset(seed=seed_val + trainer.state.epoch)\n",
    "    trainer.state.ep_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvTZVIAzxOCh"
   },
   "source": [
    "When an episode finishes, we update the running reward and perform backpropogation by calling `finish_episode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVEZBDeAUAwy"
   },
   "outputs": [],
   "source": [
    "@trainer.on(EPISODE_COMPLETED)\n",
    "def update_model():\n",
    "    trainer.state.running_reward = 0.05 * trainer.state.ep_reward + (1 - 0.05) * trainer.state.running_reward\n",
    "    finish_episode(policy, optimizer, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJV6beL4xf10"
   },
   "source": [
    "After that, every 100 (`log_interval`) episodes, we log the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1EvA2qJUFb6"
   },
   "outputs": [],
   "source": [
    "@trainer.on(EPISODE_COMPLETED(every=log_interval))\n",
    "def log_episode():\n",
    "    i_episode = trainer.state.epoch\n",
    "    print(\n",
    "        f\"Episode {i_episode}\\tLast reward: {trainer.state.ep_reward:.2f}\"\n",
    "        f\"\\tAverage length: {trainer.state.running_reward:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfa3fHTqxl2A"
   },
   "source": [
    "And finally, we check if our running reward has crossed the threshold so that we can stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzdhbUxSUHmk"
   },
   "outputs": [],
   "source": [
    "@trainer.on(EPISODE_COMPLETED)\n",
    "def should_finish_training():\n",
    "    running_reward = trainer.state.running_reward\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\n",
    "            f\"Solved! Running reward is now {running_reward} and \"\n",
    "            f\"the last episode runs to {trainer.state.timestep} time steps!\"\n",
    "        )\n",
    "        trainer.should_terminate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpU3b8ymtPCD"
   },
   "source": [
    "## Run Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjNcMaLkks8M",
    "outputId": "492f1c2d-a735-45b3-cee2-f599b5717f05"
   },
   "outputs": [],
   "source": [
    "trainer.run(timesteps, max_epochs=max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVSKBkaMSO2g"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8_G4PK8SRvX"
   },
   "source": [
    "### On Colab\n",
    "\n",
    "Finally, we can view our saved video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "wRkNUrpQt6P6",
    "outputId": "a2b1163c-5e9c-49d4-ce2e-d62424cdd0bb"
   },
   "outputs": [],
   "source": [
    "mp4list = glob.glob('video/*.mp4')\n",
    "\n",
    "if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "              </video>'''.format(encoded.decode('ascii'))))\n",
    "else: \n",
    "    print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpqXiZUsznkY"
   },
   "source": [
    "That's it! We have successfully solved the Cartpole problem!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ignite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a5181ec3af4fa9f8113ace21834b45582f57f865b42ef804eecb3fe002d1fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
